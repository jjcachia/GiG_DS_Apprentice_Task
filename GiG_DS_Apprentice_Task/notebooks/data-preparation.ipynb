{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce41f30",
   "metadata": {},
   "source": [
    "## DS Apprentice Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9dc646",
   "metadata": {},
   "source": [
    "### Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579cc632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jerem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\jerem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jerem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# download stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ead030",
   "metadata": {},
   "source": [
    "### 0. CSV file cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f48bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(os.path.join(\"..\", \"data\"))\n",
    "\n",
    "input_path = os.path.join(data_dir, \"gig_docs.csv\")\n",
    "output_path = os.path.join(data_dir, \"cleaned_gig_docs.csv\")\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", newline='', encoding=\"utf-8\") as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for row in reader:\n",
    "        # row = [cell.strip() for cell in row]\n",
    "\n",
    "        temp_row = row[:3]\n",
    "\n",
    "        # Handle Answer (merge all remaining columns)\n",
    "        answer = \", \".join(row[3:])  # Join remaining columns with a comma\n",
    "\n",
    "        temp_row.append(answer)\n",
    "        writer.writerow(temp_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be7179",
   "metadata": {},
   "source": [
    "### 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c979807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Question_Example</th>\n",
       "      <th>Answer_Snippet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>What is GiG Broker?</td>\n",
       "      <td>GiG Broker offers a list of data streams consi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>How is data consistency handled in GiG Broker?</td>\n",
       "      <td>GiG Broker guarantees eventual consistency,  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>What are enriched data streams?</td>\n",
       "      <td>Enriched data streams from Core Data are pushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>How can consumers access data from GiG Broker?</td>\n",
       "      <td>Consumers authenticate to the service and can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>What is the purpose of the CRM Payload in GiG ...</td>\n",
       "      <td>The CRM Payload is a stream tailored for CRM t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>SQL Connectivity</td>\n",
       "      <td>What options are available for users to connec...</td>\n",
       "      <td>Authorised users can connect using several opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>SQL Connectivity</td>\n",
       "      <td>What is dim_customer in the SQL Connectivity l...</td>\n",
       "      <td>dim_customer is a dimension table that stores ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>SQL Connectivity</td>\n",
       "      <td>What kind of information is in fact_bonus in t...</td>\n",
       "      <td>fact_bonus stores bonus information including ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>SQL Connectivity</td>\n",
       "      <td>What is 'dim_game_detail' in SQL Connectivity?</td>\n",
       "      <td>This dimension stores detailed game informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>SQL Connectivity</td>\n",
       "      <td>How are tables like dim_age_group structured i...</td>\n",
       "      <td>dim_age_group stores age group information wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Topic                                   Question_Example  \\\n",
       "Doc_ID                                                                        \n",
       "12            GiG Broker                                What is GiG Broker?   \n",
       "13            GiG Broker     How is data consistency handled in GiG Broker?   \n",
       "14            GiG Broker                    What are enriched data streams?   \n",
       "29            GiG Broker     How can consumers access data from GiG Broker?   \n",
       "38            GiG Broker  What is the purpose of the CRM Payload in GiG ...   \n",
       "...                  ...                                                ...   \n",
       "56      SQL Connectivity  What options are available for users to connec...   \n",
       "57      SQL Connectivity  What is dim_customer in the SQL Connectivity l...   \n",
       "58      SQL Connectivity  What kind of information is in fact_bonus in t...   \n",
       "66      SQL Connectivity     What is 'dim_game_detail' in SQL Connectivity?   \n",
       "69      SQL Connectivity  How are tables like dim_age_group structured i...   \n",
       "\n",
       "                                           Answer_Snippet  \n",
       "Doc_ID                                                     \n",
       "12      GiG Broker offers a list of data streams consi...  \n",
       "13      GiG Broker guarantees eventual consistency,  w...  \n",
       "14      Enriched data streams from Core Data are pushe...  \n",
       "29      Consumers authenticate to the service and can ...  \n",
       "38      The CRM Payload is a stream tailored for CRM t...  \n",
       "...                                                   ...  \n",
       "56      Authorised users can connect using several opt...  \n",
       "57      dim_customer is a dimension table that stores ...  \n",
       "58      fact_bonus stores bonus information including ...  \n",
       "66      This dimension stores detailed game informatio...  \n",
       "69      dim_age_group stores age group information wit...  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned gig_docs CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv(os.path.join(data_dir, \"cleaned_gig_docs.csv\"))\n",
    "\n",
    "# If Doc_ID is unique, set it as the index\n",
    "if df['Doc_ID'].is_unique:\n",
    "    df.set_index('Doc_ID', inplace=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b96ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic:\n",
      "GiG Data Segmentation: 22\n",
      "SQL Connectivity: 13\n",
      "GiG Broker: 10\n",
      "KPI Definitions: 9\n",
      "GRE: 8\n",
      "Market Packages: 8\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of documents per topic\n",
    "topic_counts = df['Topic'].value_counts()\n",
    "\n",
    "print(\"Number of documents per topic:\")\n",
    "for topic, document_count in topic_counts.items():\n",
    "   print(f\"{topic}: {document_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f11c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "Topic: 0 missing values\n",
      "Question_Example: 0 missing values\n",
      "Answer_Snippet: 0 missing values\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing values in each column:\")\n",
    "for column, count in missing_values.items():\n",
    "    print(f\"{column}: {count} missing values\")\n",
    "\n",
    "# TODO: Handle missing values if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01754b75",
   "metadata": {},
   "source": [
    "### 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed87ea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  Text cleaning? This is a test for GiG...!\n",
      " Test works well. 123, & 456.78 and \tcafé.\n",
      "\n",
      "Cleaned text: ['text', 'clean', 'test', 'game', 'innovation', 'group', 'test', 'work', 'well', '123', '45678', 'cafe']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "abbreviation_map = {\n",
    "    \"gig\": \"gaming innovation group\", \n",
    "    \"sql\": \"structured query language\",\n",
    "    \"kpi\": \"key performance indicator\",\n",
    "    \"kpis\": \"key performance indicators\",\n",
    "    \"gre\": \"game recommendation engine\",\n",
    "    \"rtp\": \"return to player\",\n",
    "    \"atpu\": \"average time per user\",\n",
    "}\n",
    "\n",
    "def prepare_text(text, remove_numbers=False, remove_stopwords=True, apply_lemmantization=True, remove_contractions=True, remove_abbreviations=True):\n",
    "    \"\"\"Prepares a string of text for search/matching\"\"\"\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input text must be a string.\")\n",
    "\n",
    "    # Clean the text (convert to lowercase, remove accents, punctuation, and special characters)\n",
    "    cleaned_text = clean_text(text, remove_numbers=remove_numbers)\n",
    "\n",
    "    # Optional: Expand abbreviations (e.g., \"gig\" → \"gaming innovation group\")\n",
    "    if remove_abbreviations:\n",
    "        pattern = r'\\b(' + '|'.join(re.escape(key) for key in abbreviation_map.keys()) + r')\\b'\n",
    "        cleaned_text = re.sub(pattern, lambda x: abbreviation_map[x.group()], cleaned_text)\n",
    "\n",
    "    # Optional: Expand contractions (e.g., \"don't\" → \"do not\", \"it's\" → \"it is\")\n",
    "    if remove_contractions:\n",
    "        cleaned_text = contractions.fix(cleaned_text) \n",
    "\n",
    "    # Split the processed text into tokens\n",
    "    tokenized_text = word_tokenize(cleaned_text)\n",
    "\n",
    "    # Optional: Remove Stopwords (e.g., and, or, but, etc.) from tokens \n",
    "    if remove_stopwords:\n",
    "        tokenized_text = [word for word in tokenized_text if word not in stop_words]\n",
    "\n",
    "    # Optional: Lemmatize the tokens (e.g., running → run, better → good)\n",
    "    if apply_lemmantization:\n",
    "        tokenized_text = lemmantize_tokens(tokenized_text)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "def clean_text(text, remove_numbers=False):\n",
    "    if not isinstance(text, str):\n",
    "        raise ValueError(\"Input text must be a string.\")\n",
    "\n",
    "    # Normalize unicode characters (e.g., café → cafe)\n",
    "    text = unidecode(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Create a set of characters to remove\n",
    "    chars_to_remove = set()\n",
    "\n",
    "    # Add punctuation characters to the set\n",
    "    chars_to_remove = chars_to_remove.union(set(string.punctuation))\n",
    "\n",
    "    # Add whitespace characters (excluding a single space) to the set\n",
    "    chars_to_remove = chars_to_remove.union(set(string.whitespace) - {' '})\n",
    "\n",
    "    # Optional: Add numbers to the set\n",
    "    chars_to_remove = chars_to_remove.union(set(string.digits)) if remove_numbers else chars_to_remove\n",
    "\n",
    "    # Remove the set of characters from the text\n",
    "    translation_table = str.maketrans('', '', ''.join(chars_to_remove))\n",
    "    text = text.translate(translation_table)\n",
    "\n",
    "    # Strip leading and trailing white spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\" Converts NLTK POS tags to WordNet POS tags. \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "def lemmantize_tokens(tokens):\n",
    "    \"\"\"Lemmatizes a list of tokens using WordNet lemmatization.\"\"\"\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \" Text cleaning? This is a test for GiG...!\\n Test works well. 123, & 456.78 and \\tcafé.\"\n",
    "cleaned_text = prepare_text(text)\n",
    "print(f\"Original text: {text}\\n\")\n",
    "print(f\"Cleaned text: {cleaned_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03954367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Question_Example</th>\n",
       "      <th>Answer_Snippet</th>\n",
       "      <th>Question_Tokenized</th>\n",
       "      <th>Answer_Tokenized</th>\n",
       "      <th>Combined_Tokenized</th>\n",
       "      <th>Combined_Token_Counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>What is GiG Broker?</td>\n",
       "      <td>GiG Broker offers a list of data streams consi...</td>\n",
       "      <td>[game, innovation, group, broker]</td>\n",
       "      <td>[game, innovation, group, broker, offer, list,...</td>\n",
       "      <td>[game, innovation, group, broker, game, innova...</td>\n",
       "      <td>{'game': 2, 'innovation': 2, 'group': 2, 'brok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>How is data consistency handled in GiG Broker?</td>\n",
       "      <td>GiG Broker guarantees eventual consistency,  w...</td>\n",
       "      <td>[data, consistency, handle, game, innovation, ...</td>\n",
       "      <td>[game, innovation, group, broker, guarantee, e...</td>\n",
       "      <td>[data, consistency, handle, game, innovation, ...</td>\n",
       "      <td>{'data': 4, 'consistency': 2, 'handle': 1, 'ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>What are enriched data streams?</td>\n",
       "      <td>Enriched data streams from Core Data are pushe...</td>\n",
       "      <td>[enrich, data, stream]</td>\n",
       "      <td>[enrich, data, stream, core, data, push, respe...</td>\n",
       "      <td>[enrich, data, stream, enrich, data, stream, c...</td>\n",
       "      <td>{'enrich': 3, 'data': 4, 'stream': 2, 'core': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>How can consumers access data from GiG Broker?</td>\n",
       "      <td>Consumers authenticate to the service and can ...</td>\n",
       "      <td>[consumer, access, data, game, innovation, gro...</td>\n",
       "      <td>[consumer, authenticate, service, tap, necessa...</td>\n",
       "      <td>[consumer, access, data, game, innovation, gro...</td>\n",
       "      <td>{'consumer': 2, 'access': 1, 'data': 4, 'game'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GiG Broker</td>\n",
       "      <td>What is the purpose of the CRM Payload in GiG ...</td>\n",
       "      <td>The CRM Payload is a stream tailored for CRM t...</td>\n",
       "      <td>[purpose, crm, payload, game, innovation, grou...</td>\n",
       "      <td>[crm, payload, stream, tailor, crm, collect, s...</td>\n",
       "      <td>[purpose, crm, payload, game, innovation, grou...</td>\n",
       "      <td>{'purpose': 1, 'crm': 3, 'payload': 3, 'game':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Topic                                   Question_Example  \\\n",
       "Doc_ID                                                                  \n",
       "12      GiG Broker                                What is GiG Broker?   \n",
       "13      GiG Broker     How is data consistency handled in GiG Broker?   \n",
       "14      GiG Broker                    What are enriched data streams?   \n",
       "29      GiG Broker     How can consumers access data from GiG Broker?   \n",
       "38      GiG Broker  What is the purpose of the CRM Payload in GiG ...   \n",
       "\n",
       "                                           Answer_Snippet  \\\n",
       "Doc_ID                                                      \n",
       "12      GiG Broker offers a list of data streams consi...   \n",
       "13      GiG Broker guarantees eventual consistency,  w...   \n",
       "14      Enriched data streams from Core Data are pushe...   \n",
       "29      Consumers authenticate to the service and can ...   \n",
       "38      The CRM Payload is a stream tailored for CRM t...   \n",
       "\n",
       "                                       Question_Tokenized  \\\n",
       "Doc_ID                                                      \n",
       "12                      [game, innovation, group, broker]   \n",
       "13      [data, consistency, handle, game, innovation, ...   \n",
       "14                                 [enrich, data, stream]   \n",
       "29      [consumer, access, data, game, innovation, gro...   \n",
       "38      [purpose, crm, payload, game, innovation, grou...   \n",
       "\n",
       "                                         Answer_Tokenized  \\\n",
       "Doc_ID                                                      \n",
       "12      [game, innovation, group, broker, offer, list,...   \n",
       "13      [game, innovation, group, broker, guarantee, e...   \n",
       "14      [enrich, data, stream, core, data, push, respe...   \n",
       "29      [consumer, authenticate, service, tap, necessa...   \n",
       "38      [crm, payload, stream, tailor, crm, collect, s...   \n",
       "\n",
       "                                       Combined_Tokenized  \\\n",
       "Doc_ID                                                      \n",
       "12      [game, innovation, group, broker, game, innova...   \n",
       "13      [data, consistency, handle, game, innovation, ...   \n",
       "14      [enrich, data, stream, enrich, data, stream, c...   \n",
       "29      [consumer, access, data, game, innovation, gro...   \n",
       "38      [purpose, crm, payload, game, innovation, grou...   \n",
       "\n",
       "                                    Combined_Token_Counts  \n",
       "Doc_ID                                                     \n",
       "12      {'game': 2, 'innovation': 2, 'group': 2, 'brok...  \n",
       "13      {'data': 4, 'consistency': 2, 'handle': 1, 'ga...  \n",
       "14      {'enrich': 3, 'data': 4, 'stream': 2, 'core': ...  \n",
       "29      {'consumer': 2, 'access': 1, 'data': 4, 'game'...  \n",
       "38      {'purpose': 1, 'crm': 3, 'payload': 3, 'game':...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    " \n",
    "# Apply the normalization function to the 'Question' and 'Answer' columns\n",
    "df['Question_Tokenized'] = df['Question_Example'].apply(prepare_text)\n",
    "df['Answer_Tokenized'] = df['Answer_Snippet'].apply(prepare_text)\n",
    "\n",
    "# Add a new column 'Combined' with the cleaned text from both 'Question' and 'Answer'\n",
    "df['Combined_Tokenized'] = df['Question_Tokenized'] + df['Answer_Tokenized']\n",
    "\n",
    "df['Combined_Token_Counts'] = df['Combined_Tokenized'].apply(Counter)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4160df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = dict(zip(df.index, df['Combined_Token_Counts']))\n",
    "\n",
    "output_pickle_path = os.path.join(data_dir, \"gig_docs_tokenized.pkl\")\n",
    "with open(output_pickle_path, 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f2ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "# Fit the vectorizer on the 'Combined_Cleaned' column\n",
    "matrix = vectorizer.fit_transform(df['Combined_Tokenized'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "output_pickle_path = os.path.join(data_dir, \"countvector_data.pkl\")\n",
    "with open(output_pickle_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"matrix\": matrix,\n",
    "        \"doc_ids\": df.index.tolist()\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d652644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit and transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Combined_Tokenized'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "output_pickle_path = os.path.join(data_dir, \"tfidf_data.pkl\")\n",
    "with open(output_pickle_path, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"matrix\": tfidf_matrix,\n",
    "        \"doc_ids\": df.index.tolist()\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
